{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para correr la carga de los files y el filtro de personas correr abajo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person        19414\n",
      "label         19414\n",
      "frecuencia    19414\n",
      "dtype: int64\n",
      "0    18434\n",
      "1      980\n",
      "Name: label, dtype: int64\n",
      "person        16757\n",
      "label         16757\n",
      "frecuencia    16757\n",
      "dtype: int64\n",
      "0    15777\n",
      "1      980\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "personas_entrenar = pd.read_csv('data/labels_training_set.csv', low_memory=False)\n",
    "informacion = pd.read_csv('data/events_up_to_01062018.csv', low_memory= False)\n",
    "personas_clasificar = pd.read_csv('data/trocafone_kaggle_test.csv', low_memory = False)\n",
    "\n",
    "info = personas_entrenar.merge(informacion, on = 'person', how = 'inner')\n",
    "info = info[['person']]\n",
    "info['frecuencia'] = 1\n",
    "data = pd.DataFrame({'frecuencia': info.groupby('person').aggregate(sum)['frecuencia']}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "personas_entrenar_frec = personas_entrenar.merge(data, on = 'person', how = 'left')\n",
    "# personas antes de filtrar \n",
    "print(personas_entrenar_frec.count())\n",
    "print(personas_entrenar_frec.label.value_counts())\n",
    "\n",
    "personas_entrenar_frec= personas_entrenar_frec[(personas_entrenar_frec['label'] == 1) | ((personas_entrenar_frec['frecuencia'] < 100) & (personas_entrenar_frec['label'] == 0))]\n",
    "\n",
    "print(personas_entrenar_frec.count())\n",
    "print(personas_entrenar_frec.label.value_counts())\n",
    "personas_entrenar=personas_entrenar_frec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_info(data, features, separacion):\n",
    "    \"\"\"funcion que prepocesa el training set\n",
    "    \n",
    "    data es el dataframe completo que le saco los features \n",
    "    \n",
    "    separacion es para separar train de validation \n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.fillna(0)\n",
    "    data_dic = data[features]  \n",
    "    \n",
    "    \n",
    "    data_set_list = data_dic.values.tolist()\n",
    "    dic = {}\n",
    "    dic['0'] = 0\n",
    "    dic_inverso = {}\n",
    "    i = 32\n",
    "    \n",
    "    for lista in data_set_list:\n",
    "        for element in lista:\n",
    "            if element not in dic:\n",
    "                    dic[element]= i\n",
    "                    dic_inverso[str(i)] = element\n",
    "                    i+=1\n",
    "            \n",
    "                \n",
    "    data[features] = data[features].applymap(lambda x: dic[str(x)])\n",
    "    data= data[['timestamp','label'] + features]\n",
    "    data['timestamp'] = pd.to_datetime(data.timestamp, format = '%Y-%m-%d %H:%M:%S')\n",
    "    data[\"dia\"] = data['timestamp'].apply(lambda x: x.day)\n",
    "    \n",
    "    del data['timestamp']\n",
    "    \n",
    "    \n",
    "    for x in range(5):\n",
    "        data= data.sample(frac=1)\n",
    "\n",
    "    Y = np.array(data['label'])\n",
    "   \n",
    "    del data ['label']\n",
    " \n",
    "    \n",
    "    \n",
    "    largo = int(len(data.index)*separacion)\n",
    "    \n",
    "    X = data.values\n",
    "    \n",
    "    train_x = X[:largo]\n",
    "    train_y = Y[:largo]\n",
    "    validation_x = X[largo:]\n",
    "    validation_y = Y[largo:]\n",
    "\n",
    "    return train_x, train_y, validation_x, validation_y, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_predic(data, features, dic):\n",
    "    \"\"\"\n",
    "    preprocesa el set a predecir\n",
    "    aca lo mismo pero que para el train pero sin el label\n",
    "    recibe el dic viejo con los valores que use en el train.\n",
    "    \n",
    "    si el valor no esta en el dic que viene de train lo agrega.\n",
    "    \n",
    "    esto pasa porque saco a muchos que tienen cero en label por lo que sus \n",
    "    atributos no entran al dic antes\n",
    "    \n",
    "    i arranca en 1000 para que tenga peso que es un atributo que viene de label 0 \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data = data[['timestamp'] + features]\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    data_set_list = data.values.tolist()\n",
    "       \n",
    "    #nota de color:     \n",
    "    i = 1000\n",
    "    \n",
    "    for lista in data_set_list:\n",
    "        for element in lista:\n",
    "            if element not in dic:\n",
    "                    dic[element]= i\n",
    "                    i+=1\n",
    "            \n",
    "               \n",
    "\n",
    "    data[features] = data[features].applymap(lambda x: dic[str(x)])\n",
    "    data['timestamp'] = pd.to_datetime(data.timestamp, format = '%Y-%m-%d %H:%M:%S')\n",
    "    data[\"dia\"] = data['timestamp'].apply(lambda x: x.day)\n",
    "  \n",
    "    del data['timestamp']\n",
    "\n",
    "    return data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca hace el train de ambos data set y los predice\n",
    "\n",
    "guarda el resultado en data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24929\n",
      "105809\n",
      "# score: 0.8184613501812786\n",
      "fin del training\n",
      "0    10129\n",
      "1     9286\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data = personas_entrenar.merge(informacion, on = 'person', how = 'inner')\n",
    "\n",
    "#aca podes elegir los features \n",
    "features = ['event', 'model','condition','storage','color','city','region','country','new_vs_returning','device_type','channel']\n",
    "\n",
    "train_x, train_y, validation_x, validation_y,dic = preprocesar_info(train_data, features,0.75)\n",
    "\n",
    "#imprimo cantidad de 1s y 0s de los validation \n",
    "print(np.count_nonzero(validation_y == 1))\n",
    "print(np.count_nonzero(validation_y == 0))\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "clf.fit(train_x, train_y) \n",
    "\n",
    "pred = clf.predict(validation_x)\n",
    "\n",
    "\n",
    "print (\"# \" + \"score\" + \": \" + str(accuracy_score(validation_y, pred)))\n",
    "print(\"fin del training\")\n",
    "\n",
    "\n",
    "\n",
    "prediccion_set = personas_clasificar.merge(informacion, on = 'person', how = 'inner')\n",
    "inputs = preprocesar_predic(prediccion_set, features, dic)\n",
    "prediccion_set['label'] =  clf.predict(inputs)\n",
    "\n",
    "\n",
    "\n",
    "result = personas_clasificar.merge(prediccion_set, how='inner', on='person')\n",
    "result = result[['person', 'label']]\n",
    "result['label'] = result['label'].astype('int')\n",
    "result = result.groupby('person').sum()\n",
    "result.reset_index(inplace=True)\n",
    "result['label'] = result['label'].apply(lambda x: 1 if x > 0 else 0)\n",
    "print(result.label.value_counts())\n",
    "result.to_csv('data/Decision_tree4.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esto no lo llegue a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 20):    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i) \n",
    "    knn.fit(train_x, train_y)                   \n",
    "    pred = knn.predict(validation_x)              \n",
    "    print (\"# \" + str(i) + \": \" + str(accuracy_score(validation_y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
