{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para correr la carga de los files y el filtro de personas correr abajo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person        19414\n",
      "label         19414\n",
      "frecuencia    19414\n",
      "dtype: int64\n",
      "0    18434\n",
      "1      980\n",
      "Name: label, dtype: int64\n",
      "person        18738\n",
      "label         18738\n",
      "frecuencia    18738\n",
      "dtype: int64\n",
      "0    17758\n",
      "1      980\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "personas_entrenar = pd.read_csv('data/labels_training_set.csv', low_memory=False)\n",
    "informacion = pd.read_csv('data/events_up_to_01062018.csv', low_memory= False)\n",
    "personas_clasificar = pd.read_csv('data/trocafone_kaggle_test.csv', low_memory = False)\n",
    "\n",
    "info = personas_entrenar.merge(informacion, on = 'person', how = 'inner')\n",
    "info = info[['person']]\n",
    "info['frecuencia'] = 1\n",
    "data = pd.DataFrame({'frecuencia': info.groupby('person').aggregate(sum)['frecuencia']}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "personas_entrenar_frec = personas_entrenar.merge(data, on = 'person', how = 'left')\n",
    "# personas antes de filtrar \n",
    "print(personas_entrenar_frec.count())\n",
    "print(personas_entrenar_frec.label.value_counts())\n",
    "\n",
    "personas_entrenar_frec= personas_entrenar_frec[((personas_entrenar_frec['label'] == 1)) | ((personas_entrenar_frec['frecuencia'] < 260) & (personas_entrenar_frec['label'] == 0))]\n",
    "\n",
    "print(personas_entrenar_frec.count())\n",
    "print(personas_entrenar_frec.label.value_counts())\n",
    "personas_entrenar=personas_entrenar_frec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_info(data, features, separacion):\n",
    "    \"\"\"funcion que prepocesa el training set\n",
    "    \n",
    "    data es el dataframe completo que le saco los features \n",
    "    \n",
    "    separacion es para separar train de validation \n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.fillna(0)\n",
    "    data_dic = data[features]  \n",
    "    \n",
    "    \n",
    "    data_set_list = data_dic.values.tolist()\n",
    "    dic = {}\n",
    "    dic['0'] = 0\n",
    "    dic_inverso = {}\n",
    "    i = 32\n",
    "    \n",
    "    for lista in data_set_list:\n",
    "        for element in lista:\n",
    "            if element not in dic:\n",
    "                    dic[element]= i\n",
    "                    dic_inverso[str(i)] = element\n",
    "                    i+=1\n",
    "            \n",
    "                \n",
    "    data[features] = data[features].applymap(lambda x: dic[str(x)])\n",
    "    data= data[['timestamp','label','sku'] + features]\n",
    "    data['timestamp'] = pd.to_datetime(data.timestamp, format = '%Y-%m-%d %H:%M:%S')\n",
    "    data[\"dia\"] = data['timestamp'].apply(lambda x: x.day)\n",
    "    \n",
    "    del data['timestamp']\n",
    "    \n",
    "    \n",
    "    for x in range(5):\n",
    "        data= data.sample(frac=1)\n",
    "\n",
    "    Y = np.array(data['label'])\n",
    "   \n",
    "    del data ['label']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    largo = int(len(data.index)*separacion)\n",
    "    \n",
    "    X = data.values\n",
    "    \n",
    "    #scaler = StandardScaler()\n",
    "    #scaler.fit(X)\n",
    "    #X = scaler.transform(X)\n",
    "    \n",
    "    train_x = X[:largo]\n",
    "    train_y = Y[:largo]\n",
    "    validation_x = X[largo:]\n",
    "    validation_y = Y[largo:]\n",
    "\n",
    "    return train_x, train_y, validation_x, validation_y, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_predic(data, features, dic):\n",
    "    \"\"\"\n",
    "    preprocesa el set a predecir\n",
    "    aca lo mismo pero que para el train pero sin el label\n",
    "    recibe el dic viejo con los valores que use en el train.\n",
    "    \n",
    "    si el valor no esta en el dic que viene de train lo agrega.\n",
    "    \n",
    "    esto pasa porque saco a muchos que tienen cero en label por lo que sus \n",
    "    atributos no entran al dic antes\n",
    "    \n",
    "    i arranca en 1000 para que tenga peso que es un atributo que viene de label 0 \n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.fillna(0)\n",
    "    data_to_dic = data[features]\n",
    "    \n",
    "    data_set_list = data_to_dic.values.tolist()\n",
    "       \n",
    "    #nota de color:     \n",
    "    i = 1000\n",
    "    \n",
    "    for lista in data_set_list:\n",
    "        for element in lista:\n",
    "            if element not in dic:\n",
    "                    dic[element]= i\n",
    "                    i+=1\n",
    "            \n",
    "               \n",
    "\n",
    "    data[features] = data[features].applymap(lambda x: dic[str(x)])\n",
    "    data = data[['timestamp','sku'] + features] \n",
    "    data['timestamp'] = pd.to_datetime(data.timestamp, format = '%Y-%m-%d %H:%M:%S')\n",
    "    data[\"dia\"] = data['timestamp'].apply(lambda x: x.day)\n",
    "  \n",
    "    del data['timestamp']\n",
    "    X = data.values\n",
    "    \n",
    "    #scaler = StandardScaler()\n",
    "    #scaler.fit(X)\n",
    "    #X = scaler.transform(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca hace el train de ambos data set y los predice\n",
    "\n",
    "guarda el resultado en data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25064\n",
      "181524\n",
      "5805\n",
      "200783\n",
      "# score: 0.8774711019033051\n",
      "fin del training\n",
      "0    12602\n",
      "1     6813\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data = personas_entrenar.merge(informacion, on = 'person', how = 'inner')\n",
    "\n",
    "#aca podes elegir los features \n",
    "features = ['event', 'model','condition','storage','color','city','region','country','new_vs_returning','device_type','channel', 'operating_system_version', 'browser_version' ]\n",
    "\n",
    "train_x, train_y, validation_x, validation_y,dic = preprocesar_info(train_data, features,0.75)\n",
    "\n",
    "#imprimo cantidad de 1s y 0s de los validation \n",
    "print(np.count_nonzero(validation_y == 1))\n",
    "print(np.count_nonzero(validation_y == 0))\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(train_x, train_y) \n",
    "\n",
    "pred = clf.predict(validation_x)\n",
    "\n",
    "print(np.count_nonzero(pred == 1))\n",
    "print(np.count_nonzero(pred == 0))\n",
    "\n",
    "\n",
    "print (\"# \" + \"score\" + \": \" + str(accuracy_score(validation_y, pred)))\n",
    "print(\"fin del training\")\n",
    "\n",
    "\n",
    "\n",
    "prediccion_set = personas_clasificar.merge(informacion, on = 'person', how = 'inner')\n",
    "inputs = preprocesar_predic(prediccion_set, features, dic)\n",
    "prediccion_set['label'] =  clf.predict(inputs)\n",
    "\n",
    "\n",
    "\n",
    "result = personas_clasificar.merge(prediccion_set, how='inner', on='person')\n",
    "result = result[['person', 'label']]\n",
    "result['label'] = result['label'].astype('int')\n",
    "result = result.groupby('person').sum()\n",
    "result.reset_index(inplace=True)\n",
    "result['label'] = result['label'].apply(lambda x: 1 if x > 0 else 0)\n",
    "print(result.label.value_counts())\n",
    "result.to_csv('data/Decision_tree5.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esto no lo llegue a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25064\n",
      "181524\n",
      "24168\n",
      "182420\n",
      "# 1: 0.817472457257924\n",
      "25064\n",
      "181524\n",
      "5710\n",
      "200878\n",
      "# 2: 0.8748233198443278\n",
      "25064\n",
      "181524\n",
      "11073\n",
      "195515\n",
      "# 3: 0.8644403353534571\n",
      "25064\n",
      "181524\n",
      "4940\n",
      "201648\n",
      "# 4: 0.8783762851666118\n",
      "25064\n",
      "181524\n",
      "7551\n",
      "199037\n",
      "# 5: 0.8739278176854416\n",
      "25064\n",
      "181524\n",
      "4107\n",
      "202481\n",
      "# 6: 0.8799204213216644\n",
      "25064\n",
      "181524\n",
      "5998\n",
      "200590\n",
      "# 7: 0.8764884698046354\n",
      "25064\n",
      "181524\n",
      "3454\n",
      "203134\n",
      "# 8: 0.8801382461711232\n",
      "25064\n",
      "181524\n",
      "5402\n",
      "201186\n",
      "# 9: 0.876653048579782\n",
      "25064\n",
      "181524\n",
      "3350\n",
      "203238\n",
      "# 10: 0.8797025964722055\n",
      "25064\n",
      "181524\n",
      "4149\n",
      "202439\n",
      "# 11: 0.8786715588514338\n",
      "25064\n",
      "181524\n",
      "2849\n",
      "203739\n",
      "# 12: 0.8798236102774605\n",
      "25064\n",
      "181524\n",
      "3451\n",
      "203137\n",
      "# 13: 0.8789619919840456\n",
      "25064\n",
      "181524\n",
      "2426\n",
      "204162\n",
      "# 14: 0.8803705926772126\n",
      "25064\n",
      "181524\n",
      "2899\n",
      "203689\n",
      "# 15: 0.8797267992332566\n",
      "25064\n",
      "181524\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 20):    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    print(np.count_nonzero(validation_y == 1))\n",
    "    print(np.count_nonzero(validation_y == 0))\n",
    "    knn.fit(train_x, train_y)                   \n",
    "    pred = knn.predict(validation_x)\n",
    "    print(np.count_nonzero(pred == 1))\n",
    "    print(np.count_nonzero(pred == 0))\n",
    "    print (\"# \" + str(i) + \": \" + str(accuracy_score(validation_y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
